import os
import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import sentencepiece as spm
import pyarrow.parquet as pq
import pyarrow.dataset as ds
from huggingface_hub import hf_hub_download

# GPU ë©”ëª¨ë¦¬ ê·¸ë¡œìŠ¤ ì„¤ì • (í•„ìˆ˜ ì•„ë‹˜, ì„ íƒì‚¬í•­)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU {len(gpus)}ê°œ ë©”ëª¨ë¦¬ ê·¸ë¡œìŠ¤ ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        print("âš ï¸ GPU ë©”ëª¨ë¦¬ ê·¸ë¡œìŠ¤ ì„¤ì • ì‹¤íŒ¨:", e)
else:
    print("âš ï¸ GPU ë””ë°”ì´ìŠ¤ ì—†ìŒ")

# ê²½ë¡œ ì„¤ì • (Huggingface hubì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ)
hf_token = os.getenv("HF_TOKEN")
SP_MODEL_PATH = hf_hub_download(repo_id="Yuchan5386/ELM", filename="ko_bpe.model", repo_type="model", token=hf_token)
MODEL_PATH = hf_hub_download(repo_id="Yuchan5386/ELM", filename="sentence_encoder_model.keras", repo_type="model", token=hf_token)
PARQUET_PATH = hf_hub_download(repo_id="Yuchan5386/Chat2", filename="dataset.parquet", repo_type="dataset", token=hf_token)

MAX_SEQ_LEN = 128
BATCH_SIZE = 512  # ìŠ¤íŠ¸ë¦¬ë°ìš©ìœ¼ë¡œ ì¤„ì„

# SentencePiece ë¡œë“œ
sp = spm.SentencePieceProcessor()
sp.load(SP_MODEL_PATH)

# ì»¤ìŠ¤í…€ L2Norm ë ˆì´ì–´ ì •ì˜ (ëª¨ë¸ ë¶ˆëŸ¬ì˜¬ ë•Œ í•„ìš”)
class L2NormLayer(tf.keras.layers.Layer):
    def __init__(self, axis=1, epsilon=1e-10, **kwargs):
        super().__init__(**kwargs)
        self.axis = axis
        self.epsilon = epsilon
    def call(self, inputs):
        return tf.math.l2_normalize(inputs, axis=self.axis, epsilon=self.epsilon)
    def get_config(self):
        return {"axis": self.axis, "epsilon": self.epsilon, **super().get_config()}

# ì¸ì½”ë” ëª¨ë¸ ë¡œë“œ
encoder = load_model(MODEL_PATH, custom_objects={"L2NormLayer": L2NormLayer})
print("âœ… ì¸ì½”ë” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

def sp_tokenize(texts):
    encoded = sp.encode(texts, out_type=int)
    padded = tf.keras.preprocessing.sequence.pad_sequences(
        encoded, maxlen=MAX_SEQ_LEN, padding='post', truncating='post'
    )
    return padded

def parquet_streaming_generator(parquet_path, batch_size):
    dataset = ds.dataset(parquet_path, format="parquet")
    scanner = ds.Scanner.from_dataset(dataset, batch_size=batch_size)
    
    answers = []
    for record_batch in scanner.to_batches():
        batch_df = record_batch.to_pandas()
        for conv_json in batch_df['conversation']:
            obj = json.loads(conv_json)
            conv_list = obj.get("conversation", {}).get("conversations", [])
            for turn in conv_list:
                if turn.get('from') == 'gpt' and 'value' in turn:
                    answers.append(turn['value'])
                    if len(answers) == batch_size:
                        yield sp_tokenize(answers)
                        answers = []
    if answers:
        yield sp_tokenize(answers)

def encode_sentences_streaming(parquet_path):
    embeddings_list = []
    for a_seq in parquet_streaming_generator(parquet_path, BATCH_SIZE):
        dataset = tf.data.Dataset.from_tensor_slices(a_seq).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
        embs = encoder.predict(dataset, verbose=0)
        embeddings_list.append(embs)
        print(f"ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ, ëˆ„ì  ì„ë² ë”© ìˆ˜: {sum([e.shape[0] for e in embeddings_list])}")
    embeddings = np.vstack(embeddings_list)
    return embeddings.astype('float32')

if __name__ == "__main__":
    print("ğŸ§  ì„ë² ë”© ìŠ¤íŠ¸ë¦¬ë° ê³„ì‚° ì‹œì‘...")
    answer_embs = encode_sentences_streaming(PARQUET_PATH)
    print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: shape={answer_embs.shape}")

    print("ğŸ’¾ ì„ë² ë”© ì••ì¶• ì €ì¥ ì‹œì‘...")
    np.savez_compressed('answer_embeddings_streaming.npz', embeddings=answer_embs)
    print("âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: answer_embeddings_streaming.npz")
