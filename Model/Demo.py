from flask import Flask, request, Response
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras import layers
import sentencepiece as spm
import numpy as np
import json
import time
import re
import random
import os
import faiss
from sklearn.metrics.pairwise import cosine_similarity
from huggingface_hub import hf_hub_download

app = Flask(__name__, static_folder="static")

# === Custom layers ===
class L2NormLayer(layers.Layer):
    def __init__(self, axis=1, epsilon=1e-10, **kwargs):
        super().__init__(**kwargs)
        self.axis = axis
        self.epsilon = epsilon
    def call(self, inputs):
        return tf.math.l2_normalize(inputs, axis=self.axis, epsilon=self.epsilon)
    def get_config(self):
        return {"axis": self.axis, "epsilon": self.epsilon, **super().get_config()}

class MaskedGlobalAveragePooling1D(layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    def call(self, inputs, mask=None):
        if mask is None:
            return tf.reduce_mean(inputs, axis=1)
        mask = tf.cast(mask, inputs.dtype)
        mask = tf.expand_dims(mask, axis=-1)
        summed = tf.reduce_sum(inputs * mask, axis=1)
        counts = tf.reduce_sum(mask, axis=1)
        return summed / (counts + 1e-9)
    def compute_mask(self, inputs, mask=None):
        return None

# === í™˜ê²½ ë³€ìˆ˜ ë° í† í° ===
os.environ["HF_HOME"] = "/tmp/hf_cache"
hf_token = os.getenv("HF_TOKEN")

# === ìƒìˆ˜ ===
MAX_SEQ_LEN = 128
BATCH_SIZE = 1024
SIM_THRESHOLD = 0.5  # í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥

# === ëª¨ë¸, í† í¬ë‚˜ì´ì €, ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ===
SP_MODEL_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM", filename="ko_bpe.model", repo_type="model", token=hf_token)
MODEL_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM", filename="sentence_encoder_model.keras", repo_type="model", token=hf_token)
JSONL_PATH = hf_hub_download(repo_id="Yuchan5386/Kode", filename="data.jsonl", repo_type="dataset", token=hf_token)
EMBEDDINGS_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM", filename="answer_embeddings_streaming.npz", repo_type="model", token=hf_token)
INDEX_PATH = hf_hub_download(repo_id="Yuchan5386/VeELM", filename="answer_faiss_index.index", repo_type="model", token=hf_token)

# === SentencePiece ë¡œë“œ ===
sp = spm.SentencePieceProcessor()
sp.load(SP_MODEL_PATH)

# === ì¸ì½”ë” ë¡œë“œ ===
encoder = load_model(MODEL_PATH, custom_objects={"L2NormLayer": L2NormLayer, "MaskedGlobalAveragePooling1D": MaskedGlobalAveragePooling1D})

# === ë‹µë³€ í…ìŠ¤íŠ¸ ë¡œë“œ ===
def load_answers_from_jsonl(jsonl_path):
    answers = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                obj = json.loads(line)
                convs = obj.get("conversations", [])
                for turn in convs:
                    if turn.get("from") == "gpt" and "value" in turn:
                        answers.append(turn["value"])
            except json.JSONDecodeError:
                continue
    return answers

answer_texts = load_answers_from_jsonl(JSONL_PATH)

# === ì„ë² ë”© ë¡œë“œ ===
embedding_npz = np.load(EMBEDDINGS_PATH)
answer_embs = embedding_npz['embeddings'].astype('float32')

# === faiss ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë¡œë“œ/ìƒì„± ===
dim = answer_embs.shape[1]
if os.path.isfile(INDEX_PATH):
    faiss_index = faiss.read_index(INDEX_PATH)
else:
    faiss_index = faiss.IndexFlatIP(dim)      # FAISS ì¸ë±ìŠ¤ë¥¼ faiss_index ë³€ìˆ˜ì— ë‹´ìŠµë‹ˆë‹¤.
    faiss_index.add(answer_embs)
    faiss.write_index(faiss_index, INDEX_PATH)


# === í† í¬ë‚˜ì´ì§• & íŒ¨ë”© í•¨ìˆ˜ ===
def sp_tokenize(texts):
    encoded = sp.encode(texts, out_type=int)
    if isinstance(encoded[0], list):
        padded = tf.keras.preprocessing.sequence.pad_sequences(encoded, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
    else:
        padded = tf.keras.preprocessing.sequence.pad_sequences([encoded], maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
    return padded

# === ì„ë² ë”© ìƒì„± ===
def encode_sentences(texts):
    seqs = sp_tokenize(texts)
    dataset = tf.data.Dataset.from_tensor_slices(seqs).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    return encoder.predict(dataset, verbose=0).astype('float32')

def remove_invalid_unicode(text):
    return re.sub(r'[\ud800-\udfff]', '', text)

def synonym_replace(text):
    replacements = {
        "ì¢‹ì•„ìš”": ["ê´œì°®ì•„ìš”", "ë‚˜ì˜ì§€ ì•Šì•„ìš”", "ì¢‹ì§€ìš”"],
        "ë„ì™€ë“œë¦´ê²Œìš”": ["ë„ì™€ì¤„ê²Œìš”", "í•¨ê»˜ í•´ë³¼ê¹Œìš”?", "ë‚´ê°€ ë„ì™€ì¤„ ìˆ˜ ìˆì–´ìš”!"],
        "ë¬¸ì œì—†ì–´ìš”": ["ê±±ì • ë§ˆì„¸ìš”", "ì•„ë¬´ ë¬¸ì œ ì—†ì–´ìš”", "ì „í˜€ ë¬¸ì œ ì—†ì–´ìš”"],
        "í•  ìˆ˜ ìˆì–´ìš”": ["ê°€ëŠ¥í•´ìš”", "í•  ìˆ˜ ìˆì§€ìš”", "ë„ì „í•´ë´ìš”"],
        "ì•Œê² ìŠµë‹ˆë‹¤": ["ì˜¤í‚¤!", "í™•ì¸í–ˆì–´ìš”", "ê·¸ë ‡ê²Œ í• ê²Œìš”!"],
        "ì •í™•í•©ë‹ˆë‹¤": ["ë”± ë§ì•„ìš”!", "ì •í™•íˆ ê·¸ê±°ì˜ˆìš”!", "ë§ì•˜ì–´ìš”!"],
        "ê°ì‚¬í•©ë‹ˆë‹¤": ["ê³ ë§ˆì›Œìš”", "ë•¡í!", "ê°ì‚¬í•´ìš”~"],
        "ì£„ì†¡í•©ë‹ˆë‹¤": ["ë¯¸ì•ˆí•´ìš”", "ì‹¤ë¡€í–ˆì–´ìš”", "ì–´ë¨¸ ì£„ì†¡í•´ìš”!"],
        "ì•ˆë…•í•˜ì„¸ìš”": ["ì•ˆë…•!", "í•˜ì´í•˜ì´~", "ì•ˆë‡½~"],
    }
    for word, candidates in replacements.items():
        if word in text:
            text = text.replace(word, random.choice(candidates))
    return text

def shuffle_sentences(text):
    sentences = re.split(r'(?<=[.!?]) +', text)
    if len(sentences) > 1:
        random.shuffle(sentences)
    return ' '.join(sentences)

def casual_tone(text):
    replacements = {
        "ì…ë‹ˆë‹¤": "ì˜ˆìš”", "í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤": "í•  ìˆ˜ ìˆì–´ìš”",
        "ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤": "ë„ì™€ì¤„ê²Œìš”", "ê°ì‚¬í•©ë‹ˆë‹¤": "ê³ ë§ˆì›Œìš”",
        "ì•Œê² ìŠµë‹ˆë‹¤": "ì˜¤ì¼€ì´~", "ë¬¸ì œ ì—†ìŠµë‹ˆë‹¤": "ë¬¸ì œ ì—†ì–´ìš”~",
        "ì£„ì†¡í•©ë‹ˆë‹¤": "ë¯¸ì•ˆí•´ìš”", "í™•ì¸í–ˆìŠµë‹ˆë‹¤": "ë´¤ì–´ìš”~",
        "ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤": "ì˜ ë¶€íƒí•´ìš”~", "ì¢‹ìŠµë‹ˆë‹¤": "ì¢‹ì•„ìš©", "ê´œì°®ìŠµë‹ˆë‹¤": "ê´œì°®ì•„ìš©"
    }
    for formal, casual in replacements.items():
        text = text.replace(formal, casual)
    return text

def drop_redundant(text):
    sentences = re.split(r'(?<=[.!?]) +', text)
    if len(sentences) > 1:
        keep = random.sample(sentences, k=max(1, len(sentences)//2))
        return ' '.join(keep)
    return text

def reverse_phrase(text):
    sentences = re.split(r'(?<=[.!?]) +', text)
    return ' '.join(sentences[::-1])

def random_cut(text):
    words = text.split()
    if len(words) > 5:
        cut_point = random.randint(5, len(words))
        return ' '.join(words[:cut_point])
    return text

def insert_random_prefix(text):
    prefixes = [
        "ì‚¬ì‹¤ ë§í•˜ìë©´,", "ê°œì¸ì ìœ¼ë¡œëŠ”,", "ì•„ë§ˆë„ ë§ì´ì§€,", "ê¸°ì–µì´ ë§ë‹¤ë©´,",
        "ìš”ì¦˜ ìƒê°ë‚˜ëŠ” ê±´ë°,", "ì´ê±´ ë‚´ ìƒê°ì¼ ë¿ì¸ë°,", "ê·¸ë‹ˆê¹Œ,", "ì†”ì§íˆ ë§í•´ì„œ,",
        "ì•„ã…‹ã…‹ ê·¼ë°,", "ì´ê±´ ì§„ì§œì¸ë°,", "ì§„ì§œë£¨ë‹¤ê°€,", "ë‚˜ë§Œ ì´ë ‡ê²Œ ìƒê°í•´?", "í—¿,"
    ]
    return f"{random.choice(prefixes)} {text}"

def add_suffix(text):
    suffixes = [
        " ë§ì£ ?", " ê·¸ë ‡ì§€ ì•Šì•„ìš”?", " ê·¸ëŸ° ê±° ê°™ì•„ìš”!", " íìŒ... ì´ìƒí•˜ì£ ?", " ã…ã…",
        " ì•„ë¬´íŠ¼ ê·¸ë ‡ìŠµë‹ˆë‹¤.", " ëŠë‚Œ ì˜¤ì£ ?", " ë­ ê·¸ëŸ° ê±°ì˜ˆìš”~", " ì´ê±° ì™„ì „ ê¿€íŒ!", " ì•„ì‹œê² ì£ ?",
        " ì§„ì‹¬ì´ì—ìš”!", " ìš”ì¦˜ ë‹¤ ì´ë˜ìš”~", " ê·¸ëƒ¥ ê·¸ë ‡ë‹¤êµ¬ìš”~"
    ]
    return text + random.choice(suffixes)

def add_emoji_or_slang(text):
    suffixes = [
        " ğŸ¤”", " ğŸ˜†", " ğŸ˜", " ğŸ« ", " ã…‹ã…‹ã…‹", " ã„¹ã…‡", " ã„´ã…‡ã„±", " ğŸ‘€", " ğŸ˜…", " ğŸ¤¯", " ğŸ˜‚", " ğŸ¤«"
    ]
    return text + random.choice(suffixes)

def mix_with_similar_response(query_emb, base_response, top_k=3):
    D, I = faiss_index.search(query_emb, top_k)
    if len(I[0]) < 2:
        return base_response
    z_idx = I[0][1]
    z_text = answer_texts[z_idx]
    x_sentences = re.split(r'(?<=[.!?]) +', base_response)
    z_sentences = re.split(r'(?<=[.!?]) +', z_text)
    if not z_sentences:
        return base_response
    n = max(1, int(len(z_sentences) * 0.4))
    selected = random.sample(z_sentences, min(n, len(z_sentences)))
    insert_point = random.randint(0, len(x_sentences))
    new_sentences = x_sentences[:insert_point] + selected + x_sentences[insert_point:]
    return ' '.join(new_sentences)

MATH_PATTERNS = [
    r'\d+[\+\-\*\/]\d+',
    r'\([^\)]+\)\([^\)]+\)',
    r'[a-zA-Z0-9]+\^[a-zA-Z0-9]+',
    r'\d+\/\d+',
    r'sqrt\(|\âˆš',
]

def contains_math(text):
    return any(re.search(pattern, text) for pattern in MATH_PATTERNS)

def solve_math_expression(expr: str) -> str:
    try:
        expr = expr.replace(' ', '').replace('Ã—', '*').replace('Ã·', '/')
        expr = re.sub(r'\)\(', ')*(', expr)

        result = None
        steps = []

        simplified = expr
        numeric_matches = re.findall(r'\((\d+[\+\-\*\/]\d+)\)', expr)
        for match in numeric_matches:
            try:
                val = eval(match)
                simplified = simplified.replace(f"({match})", str(val))
                steps.append(f"ë¨¼ì € ê´„í˜¸ ì•ˆ ê³„ì‚°: {match} = {val}")
            except:
                pass

        try:
            import sympy as sp
            expanded = sp.expand(simplified)
            original = sp.sympify(simplified)
            if str(expanded) != str(original):
                steps.append(f"ì‹ì„ ì „ê°œí•´ ë³¼ê²Œìš”: {original} â†’ {expanded}")
            result = str(expanded)
        except:
            try:
                result = str(eval(simplified))
                steps.append(f"ê³„ì‚° ê²°ê³¼: {simplified} = {result}")
            except:
                result = None

        if result:
            explanation = " ".join(steps)
            if not explanation:
                explanation = "ì´ ì‹ì„ ê³„ì‚°í•´ ë³¼ê²Œìš”."
            return f"{explanation} ìµœì¢… ê²°ê³¼ëŠ” {result}ì´ì—ìš”! âœ¨"
        else:
            return None
    except Exception:
        return None

FILTERED_SENTENCES = [
    "sklearn.preprocessingì—ì„œ StandardScalerë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.sklearn.decompositionì—ì„œ PCAë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.íŒŒì´í”„ë¼ì¸ = íŒŒì´í”„ë¼ì¸(ë‹¨ê³„=[    ('ìŠ¤ì¼€ì¼ëŸ¬', StandardScaler()),    ('pca', PCA())])",
    "ë‹¨ì–´ì™€ ê° ë‹¨ì–´ì˜ í•´ë‹¹ íšŸìˆ˜ê°€ í¬í•¨ëœ ì‚¬ì „ì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.",
    """def create_sentence(words):    sentence = ""    missing_words = []    for word in words:        if word.endswith("."):            sentence += word        else:            sentence += word + " "    if len(missing_words) > 0:        print("ë‹¤ìŒ ë‹¨ì–´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: ")        for word in missing_words:            missing_word = input(f"{word}ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")            sentence = sentence.replace(f"{word} ", f"{missing_word} ")    ë¬¸ì¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:```pythonwords = ["ì´ê²ƒ", "ì´ê²ƒ", ... ]"""
]

def filter_response(text):
    for f_text in FILTERED_SENTENCES:
        if f_text in text:
            return ""
    return text

QUERY_EXPANSION_DICT = {
    "tensorflow": "import tensorflow as tf",
    "keras": "import tensorflow.keras as keras",
    "numpy": "import numpy as np",
    "pandas": "import pandas as pd",
    "sklearn": "import sklearn"
}

def expand_query(text):
    return QUERY_EXPANSION_DICT.get(text.strip().lower(), text)

def generate_response(user_input, top_k=3, similarity_threshold=SIM_THRESHOLD):
    user_input = expand_query(user_input)

    if contains_math(user_input):
        math_response = solve_math_expression(user_input)
        if math_response:
            if random.random() < 0.6:
                math_response = casual_tone(math_response)
            if random.random() < 0.5:
                math_response = add_suffix(math_response)
            if random.random() < 0.5:
                math_response = add_emoji_or_slang(math_response)
            return math_response

    AUG = {
        synonym_replace: 0.6,
        shuffle_sentences: 0.4,
        casual_tone: 0.5,
        drop_redundant: 0.3,
        reverse_phrase: 0.2,
        random_cut: 0.4,
        insert_random_prefix: 0.3,
    }
    augmented = user_input
    for f, p in AUG.items():
        if random.random() < p:
            augmented = f(augmented)

    query_emb = encode_sentences([augmented])
    D, I = faiss_index.search(query_emb, top_k)  # index â†’ faiss_index ë¡œ ë³€ê²½
    sims = D[0]
    best_idx = I[0][0]
    best_score = sims[0]

    if best_score < similarity_threshold:
        return random.choice([
            "ìŒ... ì´ê±´ ê·¸ëƒ¥ ë„˜ê¸¸ê²Œìš”!", "ê·¸ê±´ ì˜ ëª¨ë¥´ê² ì–´ìš”. ğŸ˜…",
            "ì´ê±´ ë‚´ ì§€ì‹ ë°–ì¸ ê²ƒ ê°™ì•„ìš”.", "íŒ¨ìŠ¤! ë‹¤ë¥¸ ì§ˆë¬¸ ê°€ë³´ìê³ ~"
        ])

    best_response = answer_texts[best_idx]

    if random.random() < 0.5:
        best_response = mix_with_similar_response(query_emb, best_response)

    if random.random() < 0.5:
        best_response = casual_tone(best_response)

    if random.random() < 0.4:
        best_response = add_emoji_or_slang(best_response)

    return filter_response(best_response)


@app.route('/')
def index():
    return app.send_static_file('index.html')

@app.route('/api/chat', methods=['GET', 'POST'])
def chat_api():
    user_msg = request.json.get("message", "").strip() if request.method == "POST" else request.args.get("message", "").strip()

    if not user_msg:
        def err(): yield 'data: {"error":"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."}\n\n'
        return Response(err(), mimetype='text/event-stream')

    def gen():
        try:
            resp = generate_response(user_msg)
            for ch in resp:
                safe = json.dumps(ch)[1:-1]
                yield f'data: {{"char":"{safe}"}}\n\n'
                time.sleep(0.004096)
            yield 'data: {"done":true}\n\n'
        except Exception as e:
            yield f'data: {{"error":"{str(e)}"}}\n\n'

    return Response(gen(), mimetype='text/event-stream')

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=7860)
